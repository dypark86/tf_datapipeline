{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Datapipeline 과제\n",
    "\n",
    " data pipeline tutorial을 바탕으로 flower 데이터 세트 data pipeline 구현 및 훈련\n",
    " * flower dataset\n",
    "     * daisy, dandelion, roses, sunflowers, tulips 5클래스\n",
    "     * 다양한 크기의 3채널 jpg 파일\n",
    " * 데이터 경로 설정: data_dir flag를 사용하여 flower 데이터 세트가 저장된 경로 설정\n",
    " * 체크포인트 경로 설정: ckpt_name flag를 사용하여 tenosrflow 체크포인트가 저장될 경로 설정\n",
    " * 구현된 파이프 라인들간 성능 비교\n",
    " * 사용모델 tensorflow slim vgg 19 사용\n",
    "     * model/slim_vgg.py 참조\n",
    " * train_gen.py: generator 기반 훈련 파일 참조\n",
    " \n",
    "## 1. tf record 파일 생성\n",
    " flower dataset를 이용하여 tf record 파일 생성한다.\n",
    " * /data/make_tf_record.py\n",
    "     * train 3320 파일, validation 350 파일로 나누어서 저장\n",
    "     * ImageReader: jpeg 및 3 channel 지원하도록 코드수정\n",
    "         * tf.image.decode_jpeg 함수 사용\n",
    "     * _NUM_SHARDS = 5로 설정 후 아래 파일 생성\n",
    "         * flowers_train_00000-of-00005.tfrecord\n",
    "         * flowers_train_00001-of-00005.tfrecord\n",
    "         * flowers_train_00002-of-00005.tfrecord\n",
    "         * flowers_train_00003-of-00005.tfrecord\n",
    "         * flowers_train_00004-of-00005.tfrecord\n",
    "         * flowers_validation_00000-of-00005.tfrecord\n",
    "         * flowers_validation_00001-of-00005.tfrecord\n",
    "         * flowers_validation_00002-of-00005.tfrecord\n",
    "         * flowers_validation_00003-of-00005.tfrecord\n",
    "         * flowers_validation_00004-of-00005.tfrecord\n",
    "\n",
    "\n",
    "## 2. tf.train.string_input_producer + tf.train.shuffle_batch 파이프라인 구성\n",
    " 텐서플로우 string_input_producer 와 shuffle_batch 큐를 이용하여 prefetch data pipeline을 구성한다.<br/>\n",
    " /data/data_queue.py, train.py 파일을 작성 후,\n",
    " train.py 파일을 이용하여 flower 데이터 세트 훈련을 수행한다.\n",
    " * /data/data_queue.py\n",
    "     * def inputs 함수 구현\n",
    "         * input arg: split_name, batch_size, dataset_dir, num_epochs, width, height, is_training\n",
    "         * output: images, labels\n",
    "         * tf.train.string_input_producer 큐 생성\n",
    "         * tf.train.string_input_producer 큐 read, decode\n",
    "         * image preprocessing\n",
    "             * inception_preprocessing.preprocess_image 함수 사용\n",
    "         * tf.train.shuffle_batch 큐 생성\n",
    "         * images, labels return\n",
    " * train.py\n",
    "     * data_queue.py input 함수 사용하여 train pipeline 생성\n",
    "         * split_name: \"train\"\n",
    "         * is_training: True\n",
    "     * data_queue.py input 함수 사용하여 validate pipeline 생성\n",
    "         * split_name: \"validation\"\n",
    "         * is_training: False\n",
    "     * train pipeline 생성 후, 훈련 모델과 연결 후 훈련\n",
    "     * train pipeline 1 epoch 수행 후 validate pipeline 1epoch 수행\n",
    "         * validate_run 함수 내 validate pipeline 데이터 predict 수행\n",
    " * 2 epoch 수행 후, 훈련된 데이터 수 확인\n",
    " * step 당 수행 시간 확인\n",
    "     * tf.train.shuffle_batch 큐 thread수 변경하며 수행시간 확인\n",
    "\n",
    "\n",
    "## 3. tf.contrib.dataset 파이프라인 구성\n",
    "\n",
    " * /data/dataset_queue.py\n",
    "      * def inputs 함수 구현\n",
    "         * input arg: split_name, batch_size, dataset_dir, num_epochs, is_training\n",
    "         * output: dataset\n",
    "         * TFRecordDataset 파이프라인 생성\n",
    "         * Dataset read_and_decode 함수 맵핑\n",
    "         * is_training True: dataset_preprocessing.preprocess_for_train 함수 맵핑\n",
    "         * is_training True: dataset_preprocessing.preprocess_for_eval 함수 맵핑\n",
    "         * dataset 파이프라인 epoch 설정\n",
    "         * dataset 파이프라인 batchsize 설정\n",
    "         * dataset 파이프라인 return\n",
    "\n",
    " * /data/dataset_preprocessing.py\n",
    "      * preprocess_for_train 함수 구현\n",
    "          * inception_preprocessing.py preprocess_for_train 함수 기반 구현\n",
    "          * input: image, label, height=224, width=224, bbox=None, fast_mode=True, scope=None\n",
    "          * output: image, label \n",
    "          * height, width 지정\n",
    "          * label input 및 output arg 추가\n",
    "          \n",
    "      * preprocess_for_eval 함수 구현\n",
    "          * inception_preprocessing.py preprocess_for_eval 함수 기반 구현\n",
    "          * input: image, label, height=224, width=224, central_fraction=0.875, scope=None\n",
    "          * output: image, label \n",
    "          * height, width 지정\n",
    "          * label input 및 output arg 추가\n",
    "\n",
    " * train_dataset.py\n",
    "     * dataset_preprocessing.py input 함수 사용하여 train dataset 생성\n",
    "         * split_name: \"train\"\n",
    "         * is_training: True\n",
    "     * dataset_preprocessing.py input 함수 사용하여 validate dataset 생성\n",
    "         * split_name: \"validation\"\n",
    "         * is_training: False\n",
    "     * train dataset 생성 후, 훈련 모델과 연결 후 훈련\n",
    "     * train dataset 1 epoch 수행 후 validate dataset 1epoch 수행\n",
    "         * validate_run 함수 내 validate dataset 데이터 predict 수행\n",
    " * 2 epoch 수행 후, 훈련된 데이터 수 확인\n",
    " * step 당 수행 시간 확인\n",
    "\n",
    "\n",
    "## 4. tf.contrib.dataset prefetch 파이프라인 구성 (추가과제)\n",
    " tf.contrib.dataset와 tf.train.shuffle_batch 큐를 이용하여 tf.contrib.dataset prefetch 파이프라인을 구성 \n",
    " - dataset 생성 후 dataset image,label tf.train.shuffle_batch 큐에 삽입\n",
    " * /data/dataset_queue_prefetch.py\n",
    "     * def inputs 함수 구현\n",
    "         * input arg: split_name, batch_size, dataset_dir, num_epochs, is_training\n",
    "         * output: images, labels\n",
    "         * TFRecordDataset 파이프라인 생성\n",
    "         * Dataset read_and_decode 함수 맵핑\n",
    "         * is_training True: dataset_preprocessing.preprocess_for_train 함수 맵핑\n",
    "         * is_training True: dataset_preprocessing.preprocess_for_eval 함수 맵핑\n",
    "         * dataset 파이프라인 epoch 설정\n",
    "         * dataset 파이프라인 tf.train.shuffle_batch 큐 연결\n",
    "         * images, labels 반환\n",
    " * train_dataset_prefetch.py\n",
    "     * dataset_queue_prefetch.py input 함수 사용하여 train dataset 생성\n",
    "         * split_name: \"train\"\n",
    "         * is_training: True\n",
    "     * dataset_queue_prefetch.py input 함수 사용하여 validate dataset 생성\n",
    "         * split_name: \"validation\"\n",
    "         * is_training: False\n",
    "     * train dataset 생성 후, 훈련 모델과 연결 후 훈련\n",
    "     * train dataset 1 epoch 수행 후 validate dataset 1epoch 수행\n",
    "         * validate_run 함수 내 validate dataset 데이터 predict 수행\n",
    " * 2 epoch 수행 후, 훈련된 데이터 수 확인\n",
    " * step 당 수행 시간 확인\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}